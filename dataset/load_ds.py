import os
from torch.utils.data import Dataset
from torchvision import transforms
import numpy as np
from torch import load


def get_label(concepts_list:list, args) -> int:
    """Get label for the dataset."""
    if args.custom_task:
        if type(concepts_list[0]) == np.int32:
            assert len(concepts_list) == 4, "The number of concepts is not 4"
            # assuming 4 concepts
            c1,c2,c3,c4 = concepts_list
            return 1 if c1+c2 > c3+c4 else 0
        else:
            assert len(concepts_list[0]) == 4, "The number of concepts is not 4"
            # assuming 4 concepts
            c1 = concepts_list[:,0]
            c2 = concepts_list[:,1]
            c3 = concepts_list[:,2]
            c4 = concepts_list[:,3]
            res = [1 if c1+c2 > c3+c4 else 0 for c1,c2,c3,c4 in zip(c1,c2,c3,c4)]
            return res
    else:
        if type(concepts_list[0]) == np.int32:
            assert len(concepts_list) == 2, "The number of concepts is not 2"
            c1,c2 = concepts_list
            return c1+c2
        else:
            c1 = concepts_list[:,0]
            c2 = concepts_list[:,1]
            res = [c1+c2 for c1,c2 in zip(c1,c2)]
            return res
class nMNIST(Dataset):
    """nMNIST dataset."""

    def __init__(self, split:str, data_path, args):
        print(f'Loading {split} data')
        self.data, self.labels = self.read_data(path=data_path, split=split)
        self.data = self.data
        
        # This uses the label generated by the function from the dataset
        self.targets= self.labels[:,-1:].reshape(-1)
        self.concepts= self.labels[:,:-1]
        self.real_concepts = np.copy(self.labels[:,:-1])

        # Override the label with a custom function
        self.targets = get_label(self.real_concepts, args)
        #print(self.targets)
        normalize = transforms.Normalize((0.1307,), (0.3081,))

        self.transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.ToTensor(),  # implicitly divides by 255
        # normalize
        ])

    def __len__(self):
        return self.data.shape[0]

    def __getitem__(self, idx):
            image = self.data[idx]
            label=self.targets[idx]
            concepts=self.concepts[idx]

            if self.transform is not None:
                newimage=self.transform(image.astype("uint8"))

            return newimage, label,concepts

    def read_data(self, path, split):
        """
        Returns images and labels
        """
        path += '.pt'
        try:
            print(f"Loading data from {path}")
            data = load(path)
            print("Loaded.")
        except:
            print("No dataset found.")

        #print(data)
        images = data[split]['images']
        labels = data[split]['labels']
        
        return images, labels

    def reset_counter(self):
        self.world_counter = {c: self.samples_x_world // self.batch_size for c in self.worlds}
        # self.world_counter = {c: self.samples_x_world for c in self.worlds}


class nMNIST_INTERVENTION(Dataset):
    """nMNIST dataset with 3 samples to use for interventions."""

    def __init__(self, split:str, data_path, args):
        print(f'Loading {split} data')
        self.data, self.labels = self.read_data(path=data_path, split=split)
        #self.data = self.data
        targets = []
        concepts = []
        # This uses the label generated by the function from the dataset
        for i,samp in enumerate(self.labels):
            base,source1,source2 = samp
            sample_labels = [base[-1],source1[-1],source2[-1]]
            sample_concepts = [base[:-1],source1[:-1],source2[:-1]]
            concepts.append(sample_concepts)
            sample_targets = []
            for con in sample_concepts:
                # Override the label with a custom function
                target_val = get_label(con, args)
                sample_targets.append(target_val)
            targets.append(sample_targets)
        self.targets= np.array(targets)
        self.concepts= np.array(concepts)

        
        #print(self.targets)
        normalize = transforms.Normalize((0.1307,), (0.3081,))

        self.transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.ToTensor(),  # implicitly divides by 255
        # normalize
        ])

    def __len__(self):
        return self.data.shape[0]

    def __getitem__(self, idx):
            base,s1,s2 = self.data[idx]
            label=self.targets[idx]
            concepts=self.concepts[idx]

            if self.transform is not None:
                newimageb=self.transform(base.astype("uint8"))
                newimages1=self.transform(s1.astype("uint8"))
                newimages2=self.transform(s2.astype("uint8"))

            return [newimageb,newimages1,newimages2], label,concepts

    def read_data(self, path, split):
        """
        Returns images and labels
        """
        path += '.pt'
        try:
            print(f"Loading data from {path}")
            data = load(path)
            print("Loaded.")
        except:
            print("No dataset found.")

        #print(data)
        images = data[split]['images']
        labels = data[split]['labels']
        
        return images, labels

    def reset_counter(self):
        self.world_counter = {c: self.samples_x_world // self.batch_size for c in self.worlds}
        # self.world_counter = {c: self.samples_x_world for c in self.worlds}


def load_data(data_file, data_folder, args=None):
    if data_file.endswith('_no_interventions'):
        # Prepare data
        data_path = os.path.join(data_folder, data_file)
        train_set = nMNIST(f'{args.file_name}_train', data_path=data_path, args=args)
        test_set = nMNIST(f'{args.file_name}_test', data_path=data_path, args=args)
    else:
        data_path = os.path.join(data_folder, data_file)
        train_set = nMNIST_INTERVENTION(f'{args.file_name}_train', data_path=data_path, args=args)
        test_set = nMNIST_INTERVENTION(f'{args.file_name}_test', data_path=data_path, args=args)
    
    return train_set,test_set